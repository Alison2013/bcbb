#+BLOG: bcbio
#+POSTID: 477
#+DATE: [2013-05-13 Mon 12:33]
#+BLOG: bcbio
#+TITLE: Scaling variant detection pipelines for whole genome sequencing analysis
#+CATEGORY: variation
#+TAGS: bioinformatics, variant, ngs, clinical
#+OPTIONS: toc:nil

* Scaling for whole genome sequencing

Moving from [[exome-seq][exome]] to [[wgs][whole genome sequencing]] introduces a myriad of
scaling and informatics challenges. In addition to the biological
component of correctly identifying biological variation, it's equally
important to be able to handle the informatics complexities that come
with scaling up to whole genomes, increased coverage and more samples.

At Harvard School of Public Health, we are processing an increasing
number of whole genome samples and the goal of this post is to share
experiences scaling the [[bcbio-nextgen][bcbio-nextgen]] pipeline to handle the larger
file sizes and processing times associated with these. We'll provide
an overview of the parallel architecture in bcbio-nextgen and detail
the four areas we found more useful to overcome processing
bottlenecks:

- Support heterogeneous cluster creation to maximize usage of resources.
- Increase parallelism by developing flexible methods to split and
  process by genomic regions.
- Avoid file IO and prefer streaming piped processing pipelines.
- Explore distributed file systems to better handle file IO.

This overview isn't meant as a prescription, but rather as a
description of experiences gained. We welcome suggestions and thoughts
from others working on these problems.

#+LINK: exome-seq https://en.wikipedia.org/wiki/Exome_sequencing
#+LINK: wgs https://en.wikipedia.org/wiki/Whole_genome_sequencing
#+LINK: bcbio-nextgen https://github.com/chapmanb/bcbio-nextgen

* Parallel architecture

The [[bcbio-nextgen][bcbio-nextgen]] pipeline runs in parallel on single multicore
machines or distributed on job scheduler managed clusters
like [[lsf][LSF]], [[sge][SGE]], and [[torque][TORQUE]]. The [[ipython-parallel][IPython parallel]]
framework manages setting up parallel engines and handling
communication between them. As a result, identical code can scale
from a single processor to hundreds of node on a cluster.

The high level diagram of the analysis process below shows the major
steps in the process. For whole genome samples we start with large
100Gb+ files of reads in [[fastq][FASTQ]] or [[bam][BAM]] format and perform alignment,
post-alignment processing, variant calling and variant post
processing. These steps involve numerous third party software tools
with different processing and memory requirements.

#+ATTR_HTML: alt="Variant calling overview" width="300"
[[file:variant-calling-overview.png]]

#+LINK: lsf https://en.wikipedia.org/wiki/Platform_LSF
#+LINK: sge http://gridscheduler.sourceforge.net/
#+LINK: torque https://en.wikipedia.org/wiki/TORQUE_Resource_Manager
#+LINK: ipython-parallel http://ipython.org/ipython-doc/dev/parallel/index.html
#+LINK: fastq https://en.wikipedia.org/wiki/FASTQ_format
#+LINK: bam http://samtools.sourceforge.net/SAM1.pdf
#+variant-calling-overview.png https://raw.github.com/chapmanb/bcbio-nextgen/master/docs/contents/images/variant-calling-overview.png

* Heterogeneous clusters

A major change in the pipeline was supporting heterogeneous setup of
multiple cores for different programs. This moves away from our previous
architecture, which attempted to utilize single cores throughout.
Due to algorithm restrictions, some software requires the entire set
of reads for analysis. For instance, [[bqsr][GATK's base quality recalibrator]]
uses the entire set of aligned reads to accurately calculate
positional targets for recalibration. Other software operates more
efficiently on entire files. The alignment step scales better by
running using multiple cores on a single machine, since the IO penalty
for splitting the input file is so severe.

To support this, bcbio-nextgen assesses the analysis and creates the
appropriate type of cluster environment:

- Multicore: Allocates groups of same machine processors, allowing
  analysis of individual samples with multiple cores. For example,
  running bwa with 16 cores.

- Full usage of single cores: Maximize usage of single cores for
  processes that scale beyond the number of samples. For example,
  variant calling scales across genomic regions so can utilize more
  individual cores than samples.

- Per sample single core usage: Some steps do not currently
  parallelize beyond the number of samples, so require a single core
  per sample.

Cluster schedulers allow this type of control over core usage, but an
additional future step is to include memory and disk usage
requirements as part of heterogeneous environment creation. [[aws][Amazon Web Services]]
allows selection of exact memory, disk and compute resources to match
the computational step. [[eucalyptus][Eucalyptus]] and [[openstack][OpenStack]] bring this control
to local hardware and virtual machines.

#+ATTR_HTML: alt="Variant calling overview" width="600"
[[file:parallel-clustertypes.png]]

#+LINK: bqsr http://www.broadinstitute.org/gatk/gatkdocs/org_broadinstitute_sting_gatk_walkers_bqsr_BaseRecalibrator.html
#+LINK: aws http://aws.amazon.com/
#+LINK: openstack http://www.openstack.org/
#+LINK: eucalyptus http://www.eucalyptus.com/
#+parallel-clustertypes.png https://raw.github.com/chapmanb/bcbio-nextgen/master/docs/contents/images/parallel-clustertypes.png

* Parallelism by genomic regions

Following alignment we can run analyses in parallel, split by genomic
region. Identification of variant requires processing of continuous
regions of reads together which allows local realignment algorithms to
correctly characterize closely spaced SNPs and indels. Previously,
we'd split analyses by chromsome but this has the downside of tying
analysis times to chromosome 1, the largest chromosome. The pipeline
now identifies chromosome blocks without callable reads and uses these
no read regions to fraction the genome into more uniform sections.

As a result we can process on smaller chunks of reads for time
critical parts of the process: applying base recalibration,
de-duplication, realignment and variant calling.

* Streaming pipelines

A key bottleneck throughout the pipeline is disk usage. Steps requiring
reading and writing large BAM or FASTQ files slow down dramatically
once they overburden disk IO, distributed filesystem capabilities or
ethernet connectivity between storage nodes. A practical solution to
this problem is to avoid intermediate files and use unix pipes to
stream results between processes.

We reworked our alignment approach specifically to eliminate these
issues. The previous approach took a disk centric approach that
allowed scaling out to multiple single cores in a cluster. We split
an input FASTQ or BAM file into individual chunks of reads, and then
aligned each of these chunks independently. Finally, we merged all
the individual BAMs together to produce a final BAM file to pass on
to the next step in the process. While nicely generalized, this
approach did not scale when running 50 concurrent whole genomes.

* Distributed file systems

* Timing results

#+org-html-table-data-tags: <td style="text-align:right;">
#+ATTR_HTML: border="1" style="width:100%; height:125px"
|----------------------+---------+----------+----------+----------+-----------+------------+------------|
|                      | primary | 1 sample | 1 sample | 1 sample | 6 samples | 30 samples | 30 samples |
|                      | bottle  | 16 cores | 96 cores | 96 cores |  96 cores |  480 cores |  480 cores |
|                      | neck    |   Lustre |   Lustre |      NFS |    Lustre |     Lustre |        NFS |
|----------------------+---------+----------+----------+----------+-----------+------------+------------|
| alignment            | cpu/mem |      4.3 |      4.3 |      3.9 |       4.4 |        4.5 |        6.1 |
| align post-process   | io      |      3.7 |      1.0 |      0.9 |       4.5 |        7.0 |       20.7 |
| variant calling      | cpu/mem |      2.9 |      0.5 |      0.5 |       2.0 |        3.0 |        1.8 |
| variant post-process | io      |      1.0 |      1.0 |      0.6 |       2.3 |        4.0 |        1.5 |
|----------------------+---------+----------+----------+----------+-----------+------------+------------|
#+begin_html
 <br />
#+end_html
